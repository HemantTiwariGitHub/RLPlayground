{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartpoleTest.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemantTiwariGitHub/RLPlayground/blob/main/CartpoleTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ"
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen"
      },
      "source": [
        "env = wrap_env(gym.make(\"CartPole-v1\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89"
      },
      "source": [
        "#check out the pacman action space!\n",
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nj5sjsk15IT"
      },
      "source": [
        "state = env.reset()\n",
        "print(state)\n",
        "currentState = state\n",
        "i = 0\n",
        "TotalReward = 0\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    \n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    nextState, reward, done, info = env.step(action) \n",
        "    TotalReward+=reward;\n",
        "  \n",
        "    print(i,\": \" , currentState , \" :  \" , action , \" :  \" , reward , \" : \" , nextState , \" : \" , done, \" : \" , TotalReward)\n",
        "\n",
        "    currentState = nextState\n",
        "  \n",
        "    i=i+1\n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVYlgyyIaV4H"
      },
      "source": [
        "**Starting Cartpole**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GneQ_cToaRrR"
      },
      "source": [
        "**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ7Q4Yxn0Oht"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BkkAedSSMZx"
      },
      "source": [
        "# for plumbing code\n",
        "import collections\n",
        "from collections import deque\n",
        "import pickle\n",
        "\n",
        "# the environment\n",
        "import gym\n",
        "\n",
        "# the Agent\n",
        "from Agent import CartpoleAgent22\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlDG9iTnSSde"
      },
      "source": [
        "# breakout environment\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# get size of state and action from environment\n",
        "state_size = env.observation_space.shape[0] # equal to 4 in case of cartpole \n",
        "action_size = env.action_space.n            # equal to 2 in case of cartpole\n",
        "\n",
        "# agent needs to be initialised outside the loop since the DQN\n",
        "# network will be initialised along with the agent\n",
        "agent = CartpoleAgent22(action_size=action_size, state_size=state_size)\n",
        "\n",
        "\n",
        "# to store rewards in each episode\n",
        "rewards_per_episode, episodes = [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW80H_8XSdTO"
      },
      "source": [
        "# make dir to store model weights\n",
        "if not os.path.exists(\"saved_model_weights\"):\n",
        "    os.mkdir(\"saved_model_weights\")\n",
        "\n",
        "# n_episodes\n",
        "n_episodes = 1000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poOULSEFSkyU"
      },
      "source": [
        "#### simulation starts ####\n",
        "for episode in range(n_episodes):\n",
        "\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    # reset at the start of each episode\n",
        "    state = env.reset()\n",
        "    i=0\n",
        "    while not done:\n",
        "        #env.render()\n",
        "\n",
        "        # get action for the current state and take a step in the environment\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # save the sample <s, a, r, s', done> to the replay memory\n",
        "        agent.append_sample(state, action, reward, next_state, done)\n",
        "\n",
        "        \n",
        "\n",
        "        # train after each step\n",
        "        agent.train_model()\n",
        "\n",
        "        # add reward to the total score of this episode\n",
        "        score += reward\n",
        "\n",
        " \n",
        "        #print(i,\": \" , state , \" :  \" , action , \" :  \" , reward , \" : \" , next_state , \" : \" , done, \" : \" , score)\n",
        "\n",
        "        state = next_state\n",
        "  \n",
        "        i=i+1\n",
        "\n",
        "\n",
        "\n",
        "    # store total reward obtained in this episode\n",
        "    rewards_per_episode.append(score)\n",
        "    episodes.append(episode)\n",
        "\n",
        "    # epsilon decay\n",
        "    if agent.epsilon > agent.epsilon_min:\n",
        "        agent.epsilon *= agent.epsilon_decay\n",
        "\n",
        "    # every episode:\n",
        "    print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}\".format(episode,\n",
        "                                                                         score,\n",
        "                                                                         len(agent.memory),\n",
        "                                                                         agent.epsilon))\n",
        "    # every few episodes:\n",
        "    if episode % 10 == 0:\n",
        "        # store q-values of some prespecified state-action pairs\n",
        "        # q_dict = agent.store_q_values()\n",
        "\n",
        "        # save model weights\n",
        "        agent.save_model_weights(name=\"model_weights.h5\")\n",
        "\n",
        "#### simulation complete ####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5ksxtnSS_LH"
      },
      "source": [
        "# save stuff as pickle\n",
        "def save_pickle(obj, name):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# make directory\n",
        "if not os.path.exists(\"saved_pickle_files\"):\n",
        "    os.mkdir(\"saved_pickle_files\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgfjIR_dUyv_"
      },
      "source": [
        "# save rewards_per_episode\n",
        "save_pickle(rewards_per_episode, \"saved_pickle_files/rewards_per_episode\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJPhJqSXU3xl"
      },
      "source": [
        "\n",
        "# plot results\n",
        "with open('saved_pickle_files/rewards_per_episode.pkl', 'rb') as f:\n",
        "    rewards_per_episode = pickle.load(f)\n",
        "\n",
        "plt.plot(list(range(len(rewards_per_episode))), rewards_per_episode)\n",
        "plt.xlabel(\"episode number\")\n",
        "plt.ylabel(\"reward per episode\")\n",
        "\n",
        "# save plots in saved_plots/ directory\n",
        "plt.savefig('rewards.png')\n",
        "\n",
        "print(\"Average reward of last 100 episodes is {0}\".format(np.mean(rewards_per_episode[-100:]))) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mRfqOnTU6s-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}